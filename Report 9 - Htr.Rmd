---
title: "Neural Net Exercise"
author: "Heitor Gabriel S. Monteiro"
date: "06/10/2021"
output:
  html_document:
    highlight: tango
    theme: cerulean
    number_sections: yes
    toc: yes
    toc_float:
      collapsed: no
      smooth_scroll: no
  pdf_document:
    toc: true
    number_sections: true
    highlight: tango
geometry: margin=1in
fontfamily: mathpazo
fontsize: 11pt
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Prelúdio

O pacote ensinado pelo livro do Lantz, o `neuralnet` está desatualizado e algumas funções importantes não estão funcionando mais. Aproveitando isso, resolvi fazer tudo usando o `nnet` mas dentro do `tidymodels`, para aprender a fazer já algo mais organizado e seriado.

```{r, warning=FALSE, message=FALSE}
setwd('/home/heitor/Área de Trabalho/R Projects/Análise Macro/Lab 9')
library(tidyverse)
library(tidymodels)
library(nnet)
library(NeuralNetTools)
```

# Importação e Divisão

```{r, warning=FALSE, message=FALSE}
dds     <- read_csv("concrete.csv")
slice_1 <- initial_split(dds)
train   <- training(slice_1)
test    <- testing(slice_1)
```


# Modelo

Vamos criar a estrutura geral do nosso modelo, deixando espaços livres com `tune()` por serem os parâmetros a serem testados com vários números, reiteragas vezes.

```{r, warning=FALSE, message=FALSE}
nnet_1 <- mlp(
	hidden_units = integer(tune()),
	penalty      = double(tune()),
	activation   = 'linear') %>% 
	set_mode("regression") %>% 
	set_engine("nnet", verbose = 0) 
    # traduzir para pô-lo em termos de nnet:
nnet_1 %>% translate()
```

# Tratamentos e Fórmula para Alimentar o Modelo

Defino como os dados alimentarão o modelo já descrito acima e aplico um tratamento de normalização nos dados, usando desvio da média e desvio-padrão.

```{r, warning=FALSE, message=FALSE}
recipe_1 <- recipe(strength~.,
				   data = train) %>% 
	step_normalize(all_numeric_predictors()) %>% 
	prep()

recipe_1 %>% bake(new_data=NULL)
```

# Workflow

Junto o modelo descrito e os dados tratados, formando um workflow:

```{r, warning=FALSE, message=FALSE}
nnet_1_wrkflw <- workflow() %>%
	add_model(nnet_1) %>%
	add_recipe(recipe_1)
```

# Validação Cruzada

Defino a validação cruzada em grupos de cinco, ou seja, a amostra de treino será $\frac{4}{5}$ passando por várias reamostragens.

```{r, warning=FALSE, message=FALSE}
valid_1 <- vfold_cv(train, v = 5)
```

# Treinamento

Treinaremos o modelo com vários parâmetros e selecionaremos de acordo com `ccc`: coeficiente de concordância de correlação. Mostraremos um gráfico com os parâmetros testados.

```{r, warning=FALSE, message=FALSE, fig.align='center', fig.width=9}
nnet_1_trained <- nnet_1_wrkflw %>% 
	tune_grid(resamples = valid_1,
			  grid      = 15,
			  control   = control_grid(save_pred = T),
			  metrics   = metric_set(roc_res)) 

nnet_1_trained %>% show_best(n=15)

# (6.1) Auto-plot ---
ggplot2::autoplot(nnet_1_trained)
```

# Testando

Selecionaremos o melhor modelo, usando o `ccc`.

```{r, warning=FALSE, message=FALSE}
best_tune  <- select_best(nnet_1_trained, 'ccc')
nnet_final <- nnet_1 %>%
	finalize_model(best_tune)
```

Aplicaremos esse modelo, `nnet_final` na partição feita em `slice_1` e com a organização dos dados de acordo com `recipe_1`. Vemos que conseguimos aumentar para 92.37% a correlação entre previsto e verdadeiro.

```{r, warning=FALSE, message=FALSE}
nnet_final_wrkflw <- workflow() %>% 
	add_recipe(recipe_1) %>% 
	add_model(nnet_final) %>% 
	last_fit(slice_1) %>% 
	collect_predictions()
nnet_final_wrkflw
cor(nnet_final_wrkflw$.pred, nnet_final_wrkflw$strength)
```

Por fim, faço duas ilustrações: um plot dos dados originais contra os previstos, com a estimação perfeita sendo a linha vermelha; e a representação da rede neural.

```{r, warning=FALSE, message=FALSE, fig.align='center', fig.width=9}
nnet_final_wrkflw %>% 
	select(.row, .pred, strength) %>% 
	ggplot() +
	aes(x = strength,
		y = .pred) +
	geom_point() +
	geom_abline(intercept = 0,
				slope     = 1,
				color     ='red',
				size      = .8)
```

Para fazer a representação da rede neural, precisamos traduzir o que foi feito em `mlp()` para colocar como um objeto `nnet` e fazer o gráfico.

```{r, warning=FALSE, message=FALSE, fig.align='center', fig.width=9}
nnet_final %>% translate()

nnet3 <-nnet(strength ~ cement+slag+ash+water+superplastic+coarseagg+fineagg+age,
			 size = 8, # as penalty
			 data = recipe(strength~., data = train) %>% 
			 	step_normalize(all_numeric_predictors()) %>% 
			 	prep() %>% bake(new_data=NULL),
			 decay = 0.0218099079606513,
			 verbose = 0, trace = FALSE, 
			 linout = TRUE)
nnet3 %>% plotnet()
```






